What is a Fourier Transform?
 
 
 Result 1 
 
 
Introduction
The Transformer BIBREF0 is one of the most commonly used neural network architectures in natural language processing. Layer normalization BIBREF1 plays a key role in Transformer's success. The originally designed Transformer places the layer normalization between the residual blocks, which is usually referred to as the Transformer with Post-Layer Normalization (Post-LN) BIBREF2. This architecture has achieved state-of-the-art performance in many tasks including language modeling BIBREF3, BIBREF4 and machine translation BIBREF5, BIBREF6. Unsupervised pre-trained models based on the Post-LN Transformer architecture also show impressive performance in many downstream tasks BIBREF7, BIBREF8, BIBREF9. Despite its great success, people usually need to deal with the optimization of the Post-LN Transformer more carefully than convolutional networks or other sequence-to-sequence models BIBREF10. In particular, to train the model from scratch, any gradient-based optimization approach requires a learning rate warm-up stage BIBREF0, BIBREF11: the optimization starts with an extremely small learning rate, and then gradually increases it to a pre-defined maximum value in a pre-defined number of iterations. Such a warm-up stage not only slows down the optimization process but also brings more hyperparameter tunings. BIBREF10 has shown that the final model performance is quite sensitive to the value of the maximum learning rate and the number of warm-up iterations. Tuning such sensitive hyper-parameters is costly in training large-scale models, e.g., BERT BIBREF8 or XLNet BIBREF9. In this paper, we try to alleviate this problem by finding ways to safely remove the learning rate warm-up stage. As the warm-up stage happens in the first several iterations, we investigate the optimization behavior at initialization using mean field theory BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17. According to our theoretical analysis, when putting the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, without the warm-up stage, directly using a large learning rate to those parameters can make the optimization process unstable. Using a warm-up stage and training the model with small learning rates practically avoid this problem. Extensive experiments are provided to support our theoretical findings. Our theory also shows that the layer normalization plays a crucial role in control
 
 
 Result 2 
 
 
Introduction
The Transformer BIBREF0 is one of the most commonly used neural network architectures in natural language processing. Layer normalization BIBREF1 plays a key role in Transformer's success. The originally designed Transformer places the layer normalization between the residual blocks, which is usually referred to as the Transformer with Post-Layer Normalization (Post-LN) BIBREF2. This architecture has achieved state-of-the-art performance in many tasks including language modeling BIBREF3, BIBREF4 and machine translation BIBREF5, BIBREF6. Unsupervised pre-trained models based on the Post-LN Transformer architecture also show impressive performance in many downstream tasks BIBREF7, BIBREF8, BIBREF9. Despite its great success, people usually need to deal with the optimization of the Post-LN Transformer more carefully than convolutional networks or other sequence-to-sequence models BIBREF10. In particular, to train the model from scratch, any gradient-based optimization approach requires a learning rate warm-up stage BIBREF0, BIBREF11: the optimization starts with an extremely small learning rate, and then gradually increases it to a pre-defined maximum value in a pre-defined number of iterations. Such a warm-up stage not only slows down the optimization process but also brings more hyperparameter tunings. BIBREF10 has shown that the final model performance is quite sensitive to the value of the maximum learning rate and the number of warm-up iterations. Tuning such sensitive hyper-parameters is costly in training large-scale models, e.g., BERT BIBREF8 or XLNet BIBREF9. In this paper, we try to alleviate this problem by finding ways to safely remove the learning rate warm-up stage. As the warm-up stage happens in the first several iterations, we investigate the optimization behavior at initialization using mean field theory BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17. According to our theoretical analysis, when putting the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, without the warm-up stage, directly using a large learning rate to those parameters can make the optimization process unstable. Using a warm-up stage and training the model with small learning rates practically avoid this problem. Extensive experiments are provided to support our theoretical findings. Our theory also shows that the layer normalization plays a crucial role in control
 
 
 Result 3 
 
 
\section{Introduction}

Despite the rise of graphene and other 2D materials, semiconducting single-walled carbon nanotubes (SWNT) are still regarded as strong candidates for the next generation of high-performance ultrascaled transistors~\cite{Cao_IBM_2015,IBM_2017,3D_CNT_FET} as well as for opto-electronic devices~\cite{Review_Avouris,CNT_photonics} such as chip-scale electronic-photonic platforms~\cite{Pernice_2016} or low-threshold near-infrared tunable micro-lasers~\cite{Graf_2017}. 
Engineering a quantum dot (QD) along a (suspended) semiconducting SWNT foreshadows promising opportunities in the field of quantum information processing and sensing through recently proposed schemes such as detection and manipulation of single spins via coupling to vibrational motion~\cite{Palyi_2012}, optomechanical cooling~\cite{Wilson_Rae_2012} as well as all optical manipulation of electron spins~\cite{Galland_all_optical_2008}. Furthermore, the quasi one-dimensional geometry of SWNTs allows for defining tunable p-n junctions induced by electrostatic doping through local gates~\cite{Buchs_JAP,tunable_pn_2011}. Combining a well-defined QD within such a p-n junction structure could constitute a crucial building-block for the realization of highly desirable electrically driven, on-demand single photon emitters operating at telecom wavelength, based $e.g.$ on a turnstile device architecture~\cite{turnstile_1994,turnstile_1999}.
In practice, QDs in carbon nanotubes have been reported predominantly for two different confinement structures: i) Engineered tunneling barriers at metal-nanotube contacts~\cite{Pablo04nat} and/or by gate electrodes, used \emph{e.g.} to manipulate single electron spins~\cite{Laird:2015}, ii) Unintentional localization potentials stemming from environmental disorder~\cite{Hofmann_2016}, allowing for single-photon emission mediated by localization of band-edge excitons to QD states~\cite{CNT_photonics,Hoegele_2008,Walden_Newman_2012,Hofmann_2013,Pernice_2016_2}. Both types of structures are usually operated at cryogenic temperature due to small energy scales ranging from a few to few tens of millielectronvolts.
\\
\indent Another technique for achieving confinement in SWNTs makes use of artificial defects such as covalently bound oxygen or aryl functionalization groups on the side walls of semiconducting SWNTs, inducing deep exciton trap states allowing for single-photon emission at room temperature~\cite{Htoon_2015,tunable_QD_defects}. Also, carrie
 
 
 Result 4 
 
 
	Introduction

The nuclear power industry is facing severe economic challenges in the United States. High capital costs, low electricity demand growth, and competition from cheaper sources of electricity such as natural gas and renewables have dampened the demand for new nuclear power plants and accelerated the retirement of existing reactors. As of April 2019, seven nuclear reactors had closed in the United States since 2012, and another 12 had announced that they would retire by 2025. There are currently 98 operating U.S. reactors. As aging reactors reach the end of their operating licenses in 2030 and beyond, the number of retirements is projected to increase. In addition, cost and schedule overruns have hindered recent efforts to build new nuclear units in the United States. The only power reactors currently under construction in the United States—two new units at the Vogtle nuclear plant in Georgia—are five years behind schedule and nearly double their original estimated cost.
All nuclear power in the United States is generated by light water reactors (LWRs), which were commercialized in the 1950s and early 1960s and are now used throughout most of the world. LWRs are cooled by ordinary ("light") water, which also slows ("moderates") the neutrons that maintain the nuclear fission chain reaction. Conventional LWRs are large—with 1,000 megawatts electric generating capacity (MWe) or more—in order to spread their high construction costs among the maximum possible number of kilowatt-hours of electricity over their operating lifetime.
At the same time conventional reactors are facing an uncertain future, some in Congress contend that more nuclear power plants, not fewer, are needed to help reduce U.S. greenhouse gas emissions and bring low-carbon power to the majority of the world that currently has little access to electricity. Proponents of this view argue that the key to increasing the number of nuclear power plants is investment in "advanced" nuclear technologies, which they say could overcome the economic problems, safety concerns, and other issues that have stalled the growth of conventional LWRs.
Congress enacted legislation in September 2018 that defines "advanced nuclear reactor" as "a nuclear fission reactor with significant improvements over the most recent generation of nuclear fission reactors" or a reactor using nuclear fusion ( P.L. 115-248 ). Titled the Nuclear Energy Innovation Capabilities Act of 2017 (NEICA), the law requires the Depart
 
 
 Result 5 
 
 
\section{Introduction}

Ultracold neutral plasmas studied in the laboratory offer access to a regime of plasma physics that scales to describe thermodynamic aspects of important high-energy-density systems, including strongly coupled astrophysical plasmas \cite{VanHorn,Burrows}, as well as terrestrial sources of neutrons \cite{Hinton,Ichimaru_fusion,Atzeni,Boozer} and x-ray radiation \cite{Rousse,Esarey}.  Yet, under certain conditions, low-temperature laboratory plasmas evolve with dynamics that are governed by the quantum mechanical properties of their constituent particles, and in some cases by coherence with an external electromagnetic field.   

The relevance of ultracold plasmas to such a broad scope of problems in classical and quantum many-body physics has given rise to a great deal of experimental and theoretical research on these systems since their discovery in the late 90s.  A series of reviews affords a good overview of progress in the last twenty years \cite{Gallagher,Killian_Science,PhysRept,Lyon}.  Here, we focus on the subset of ultracold neutral plasmas that form via kinetic rate processes from state-selected Rydberg gases, and emphasize in particular the distinctive dynamics found in the evolution of molecular ultracold plasmas.  

While molecular beam investigations of threshold photoionization spectroscopy had uncovered relevant effects a few years earlier \cite{Scherzer,Alt}, the field of ultracold plasma physics began in earnest with the 1999 experiment of Rolston and coworkers on metastable xenon atoms cooled in a magneto optical trap (MOT) \cite{Killian}.  

This work and many subsequent efforts tuned the photoionization energy as a means to form a plasma of very low electron temperature built on a strongly coupled cloud of ultracold ions.  Experiment and theory soon established that fast processes associated with disorder-induced heating and longer-time electron-ion collisional rate processes act to elevate the ion temperatures to around one degree Kelvin, and constrain the effective initial electron temperature to a range above 30 K \cite{Kuzmin,Hanson,Laha}.  

This apparent limit on the thermal energy of the electrons can be more universally expressed for an expanding plasma by saying that the electron correlation parameter, $\Gamma_e$, does not exceed 0.25, where, 
\begin{equation}
\Gamma_e = \frac{e^2}{4\pi \epsilon_0 a_{ws}}\frac{1}{k_B T_e}
\label{eqn:gamma_e}
\end{equation}
defines the ratio of the average unscreened elec
 
 
 Result 6 
 
 
\section*{Dynamical Behaviour of $O$ in Lattice Gases}

The dynamical behaviour of the anisotropic order parameter  $m$ [see Eq.~\eqref{eq:def-m} in the Letter] following a quench to the critical point is well described by
the Gaussian theory for all the three lattice gas models studied, $i.e.,$ driven lattice gas with either constant (IDLG) or random (RDLG) infinite drive and equilibrium lattice gas (LG). In other words, in the short-time regime, $m \sim t^{1/2}$ [see Eq. \eqref{eq:mt}] and the Binder cumulant $g$ of the lowest transverse mode [defined in Eq. \eqref{eq:binder}] is zero in this regime. The alternative order parameter $O,$ however, distinguishes between the driven (IDLG, RDLG) and the equilibrium (LG) lattice gases. 

In order to understand  this, we first write the phenomenological scaling form for $O$,  analogous to Eq. \eqref{eq:scalingass} in  the Letter,
\begin{eqnarray}
O (t, L_{\parallel} ; S_\Delta) = L_{\parallel}^{-\beta/[\nu(1+\Delta)]} \tilde f_O (t/L_{\parallel}^{z/(1+\Delta)} ; S_\Delta).\quad
\label{eq:Oscalingass}
\end{eqnarray}
We already remarked that, in the LG, this scaling form is not compatible with the prediction $O \sim t^{1/8}  L_{\parallel}^{-1/2}$ of the Gaussian theory.  However, following Ref. \cite{AS2002}, it can be argued that, at short times, the only dependence of $O$ on the system size $L_{\parallel}$ is of the form $O \sim L_\parallel^{-1/2}$ which is very well confirmed by numerical simulations. Accordingly,  the generic behaviour of $O$ can be assumed to be
\begin{eqnarray}
O \sim t^{\alpha} L_\parallel^{-1/2}, \label{eq:O}
\end{eqnarray}
where $\alpha$ is a phenomenological exponent to be determined. This, along with Eq. \eqref{eq:Oscalingass}, implies $\tilde f_O(x) \sim x^{\alpha}.$ Comparing the finite-size behaviour in Eq.~\eqref{eq:O} with Eq.~\eqref{eq:Oscalingass} one actually infers,
\begin{eqnarray}
\alpha &=& \frac{1+ \Delta -2 \beta/\nu}{2 \, (4- \eta)}. \label{eq:alpha}
\end{eqnarray}
This equation, together with the hyperscaling relation $\Delta - 2 \beta/\nu= - \eta$ in two spatial dimensions, shows that the prediction $\alpha = 1/8$ of the Gaussian theory [see Eq. \eqref{eq:Ot}] can be obtained only when $\eta=0,$ which is the case for the IDLG (exactly) and the RDLG (approximately) but not for the LG. 

On the other hand,  Eq.~\eqref{eq:alpha} predicts $\alpha = 1/10$ upon substituting the values of the critical exponents corresponding to the Ising  universality class (LG). This is con
 
 
 Result 7 
 
 
Those are some good questions. Here are my answers for you:

 
>  Why is the image rotated before applying Hough Transform?
> 
>  

 This I don't believe is MATLAB's "official example". [I just took a quick look at the documentation page for the function](https://www.mathworks.com/help/images/ref/hough.html). I believe you pulled this from another website that we don't have access to. In any case, in general it is not necessary for you to rotate the images prior to using the Hough Transform. The goal of the Hough Transform is to find lines in the image in any orientation. Rotating them should not affect the results. However, if I were to guess the rotation was performed as a preemptive measure because the lines in the "example image" were most likely oriented at a 33 degree angle clockwise. Performing the reverse rotation would make the lines more or less straight.

 
>  What do the entries in `H` represent?
> 
>  

 `H` is what is known as an [accumulator matrix](https://en.wikipedia.org/wiki/Hough_transform#Implementation). Before we get into what the purpose of `H` is and how to interpret the matrix, you need to know how the Hough Transform works. With the Hough transform, we first perform an edge detection on the image. This is done using the Canny edge detector in your case. If you recall the Hough Transform, we can parameterize a line using the following relationship:

 
```
rho = x*cos(theta) + y*sin(theta) 
```
 `x` and `y` are points in the image and most customarily they are edge points. `theta` would be the angle made from the intersection of a line drawn from the origin meeting with the line drawn through the edge point. `rho` would be the **perpendicular** distance from the origin to this line drawn through `(x, y)` at the angle `theta`. 

 Note that the equation can yield infinity many lines located at `(x, y)` so it's common to bin or discretize the total number of possible angles to a predefined amount. MATLAB by default assumes there are **180** possible angles that range from `[-90, 90)` with a sampling factor of 1. Therefore `[-90, -89, -88, ... , 88, 89]`. What you generally do is for each edge point, you search over a predefined number of angles, determine what the corresponding `rho` is. After, we count how many times you see each `rho` and `theta` pair. Here's a quick example pulled from Wikipedia:

 ![](https://upload.wikimedia.org/wikipedia/commons/3/39/Hough_transform_diagram.png)

 Source: [Wikipedia: Hough Transform](https://en
 
 
 Result 8 
 
 
The concepts underlying these kinds of transformations are more easily seen by first looking at a one dimensional case. The image [here](http://mathworld.wolfram.com/images/eps-gif/FourierSeriesSquareWave_800.gif) shows a square wave along with several of the first terms of an infinite series. Looking at it, note that if the functions for the terms are added together, they begin to approximate the shape of the square wave. The more terms you add up, the better the approximation. But, to get from an approximation to the exact signal, you have to sum an infinite number of terms. The reason for this is that the square wave is discontinuous. If you think of a square wave as a function of time, it goes from -1 to 1 in zero time. To represent such a thing requires an infinite series. Take another look at the plot of the series terms. The first is red, the second yellow. Successive terms have more "up and down" transitions. These are from the increasing frequency of each term. Sticking with the square wave as a function of time, and each series term a function of frequency there are two equivalent representations: a function of time and a function of frequency (1/time).

 In the real world, there are no square waves. Nothing happens in zero time. Audio signals, for example occupy the range 20Hz to 20KHz, where Hz is 1/time. Such things can be represented with finite series'.

 For images, the mathematics are the same, but two things are different. First, it's two dimensional. Second the notion of time makes no sense. In the 1D sense, the square wave is merely a function that gives some numerical value for for an argument that we said was time. An (static) image is a function that gives a numerical value for for every pair of row, column indeces. In other words, the image is a function of a 2D space, that being a rectangular region. A function like that can be represented in terms of its spatial frequency. To understand what spatial frequency is, consider an 8 bit grey level image and a pair of adjacent pixels. The most abrupt transistion that can occur in the image is going from 0 (say black) to 255 (say white) over the distance of 1 pixel. This corresponds directly with the highest frequency (last) term of a series representation.

 A two dimensional Fourier (or Cosine) transformation of the image results in an array of values the same size as the image, representing the same information not as a function of space, but a function of 1/space. The information is 
 
 
 Result 9 
 
 
\section{Model equations} \label{sec:equations}

In drift-fluid models the continuity equation
\begin{align}
 \frac{\partial n}{\partial t} + \nabla\cdot\left( n \vec u_E  \right) &= 0 \label{eq:generala} 
\end{align}
describes the dynamics of the electron density $n$. Here
$\vec u_E := (\hat{\vec b} \times \nabla \phi)/B$ gives the electric drift
velocity in a magnetic field $\vec B := B \hat{\vec b}$ and an electric
potential $\phi$. We neglect contributions of the diamagnetic drift~\cite{Kube2016}.




Equation~\eqref{eq:generala} is closed by invoking quasineutrality, i.e. the divergence of the ion polarization, 
the electron diamagnetic and the gravitational drift currents must vanish
\begin{align}
  \nabla\cdot\left( \frac{n}{\Omega} \left( \frac{\partial}{\partial t} 
  + \vec u_E \cdot\nabla  \right)\frac{\nabla_\perp \phi}{B}  + n\vec u_d - n\vec u_g\right) &=0
  . 
 
 
  \label{eq:generalb}
\end{align}
Here we denote 
$\nabla_\perp\phi/B := - \hat{\vec b} \times \vec u_E$, 
the electron diamagnetic drift
$\vec u_d := - T_e(\hat{\vec b} \times\nabla n ) /enB$
with the electron temperature $T_e$,
the ion gravitational drift velocity  
$\vec u_g := m_i \hat{\vec b} \times \vec g /B$
with ion mass $m_i$, and the ion gyro-frequency
$\Omega := eB/m_i$.

Combining Eq.~\eqref{eq:generalb} with Eq.~\eqref{eq:generala} yields
\begin{align}
 \frac{\partial \rho}{\partial t} + \nabla\cdot\left( \rho\vec u_E \right) + \nabla \cdot\left( n(\vec u_\psi + \vec u_d + \vec u_g) \right) &= 0\label{eq:vorticity}
\end{align}
with the polarization charge density 
$\rho = \nabla\cdot( n\nabla_\perp \phi / \Omega B)$ 
and
$\vec u_\psi := \hat{\vec b}\times \nabla\psi /B$ 
with 
$\psi:= m_i\vec u_E^2 /2e$.
We exploit this form of Eq.~\eqref{eq:generalb} in our numerical simulations.

Equations~\eqref{eq:generala} and \eqref{eq:generalb} respectively \eqref{eq:vorticity} have several invariants.
First, in Eq.~\eqref{eq:generala} the relative particle number 
$M(t) := \int \mathrm{dA}\, (n-n_0)$ is conserved over time
$\d M(t)/\d t = 0$. 
Furthermore, we integrate 
$( T_e(1+\ln n) -T_e \ln B)\partial_t n$
as well as
$-e\phi \partial_t\rho - (m_i\vec u_E^2/2+gm_ix - T_e\ln B)\partial_t n$ 
over the domain to get, disregarding boundary contributions,
\begin{align}
  \frac{\d}{\d t}\left[T_eS(t) + H(t) \right] = 0, \label{eq:energya}\\ 
    \frac{\d}{\d t} \left[ E(t) - G(t) - H(t)\right] =  0,
    \label{eq:energyb}
\end{align}
where we define 
the entropy
$S(t):=\int \math
 
 
 Result 10 
 
 
Alright. From a theoretical point of view, given that the distortion is "arbitrary", and any solution requires you to model this arbitrary distortion, you obviously can't get an "answer". However, any solution is going to involve imposing (usually implicitly) some model of the distortion that may or may not reflect the reality of the situation.

 Since you seem to be most interested in models that presume some sort of local continuity of the distortion mapping, the most obvious choice is the one you've already tried: linear interpolaton between the nearest points. Going beyond that is going to require more sophisticated mathematical and numerical analysis knowledge.

 You are incorrect, however, in presuming you cannot expand this to more points. You can by using a least-squared error approach. Find the linear answer that minimizes the error of the other points. This is probably the most straight-forward extension. In other words, take the 5 nearest points and try to come up with a linear approximation that minimizes the error of those points. And use that. I would try this next.

 If that doesn't work, then the assumption of linearity over the area of N points is broken. At that point you'll need to upgrade to either a quadratic or cubic model. The math is going to get hectic at that point.

